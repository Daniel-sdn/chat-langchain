{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "from typing import Any, Callable, Generator, Iterable, List, Optional\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from lxml import etree\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "\n",
    "\"\"\"Abstract interface for document loader implementations.\"\"\"\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, Dict, Iterator, List, Optional, Sequence, Union\n",
    "from bs4 import BeautifulSoup, Doctype, NavigableString, Tag\n",
    "from typing import Generator\n",
    "\n",
    "from __future__ import annotations\n",
    "from abc import ABC, abstractmethod\n",
    "from functools import partial\n",
    "from langchain.load.serializable import Serializable\n",
    "from langchain.pydantic_v1 import Field\n",
    "\n",
    "from langchain.load.serializable import Serializable\n",
    "from langchain.pydantic_v1 import Field\n",
    "from typing import Any, Literal, Sequence\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, TextSplitter\n",
    "from langchain.document_loaders.blob_loaders import Blob\n",
    "\n",
    "from langchain.utils.html import PREFIXES_TO_IGNORE_REGEX, SUFFIXES_TO_IGNORE_REGEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "WEAVIATE_URL = os.environ[\"WEAVIATE_URL\"]\n",
    "WEAVIATE_API_KEY = os.environ[\"WEAVIATE_API_KEY\"]\n",
    "RECORD_MANAGER_DB_URL = os.environ[\"RECORD_MANAGER_DB_URL\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classe Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    async def atransform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n      Asynchronously transform a list of documents.\\n\\n        Args:\\n            documents: A sequence of Documents to be transformed.\\n\\n        Returns:\\n            A list of transformed Documents.\\n    \\n        return await asyncio.get_running_loop().run_in_executor(\\n            None, partial(self.transform_documents, **kwargs), documents\\n        )'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Document(Serializable):\n",
    "    \"\"\"Class for storing a piece of text and associated metadata.\"\"\"\n",
    "\n",
    "    page_content: str\n",
    "    \"\"\"String text.\"\"\"\n",
    "    metadata: dict = Field(default_factory=dict)\n",
    "    \"\"\"Arbitrary metadata about the page content (e.g., source, relationships to other\n",
    "        documents, etc.).\n",
    "    \"\"\"\n",
    "    type: Literal[\"Document\"] = \"Document\"\n",
    "\n",
    "    @classmethod\n",
    "    def is_lc_serializable(cls) -> bool:\n",
    "        \"\"\"Return whether this class is serializable.\"\"\"\n",
    "        return True\n",
    "\n",
    "\n",
    "class BaseDocumentTransformer(ABC):\n",
    "    \"\"\"Abstract base class for document transformation systems.\n",
    "\n",
    "    A document transformation system takes a sequence of Documents and returns a\n",
    "    sequence of transformed Documents.\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "            class EmbeddingsRedundantFilter(BaseDocumentTransformer, BaseModel):\n",
    "                embeddings: Embeddings\n",
    "                similarity_fn: Callable = cosine_similarity\n",
    "                similarity_threshold: float = 0.95\n",
    "\n",
    "                class Config:\n",
    "                    arbitrary_types_allowed = True\n",
    "\n",
    "                def transform_documents(\n",
    "                    self, documents: Sequence[Document], **kwargs: Any\n",
    "                ) -> Sequence[Document]:\n",
    "                    stateful_documents = get_stateful_documents(documents)\n",
    "                    embedded_documents = _get_embeddings_from_stateful_docs(\n",
    "                        self.embeddings, stateful_documents\n",
    "                    )\n",
    "                    included_idxs = _filter_similar_embeddings(\n",
    "                        embedded_documents, self.similarity_fn, self.similarity_threshold\n",
    "                    )\n",
    "                    return [stateful_documents[i] for i in sorted(included_idxs)]\n",
    "\n",
    "                async def atransform_documents(\n",
    "                    self, documents: Sequence[Document], **kwargs: Any\n",
    "                ) -> Sequence[Document]:\n",
    "                    raise NotImplementedError\n",
    "\n",
    "    \"\"\"  # noqa: E501\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform_documents(\n",
    "        self, documents: Sequence[Document], **kwargs: Any\n",
    "    ) -> Sequence[Document]:\n",
    "        \"\"\"Transform a list of documents.\n",
    "\n",
    "        Args:\n",
    "            documents: A sequence of Documents to be transformed.\n",
    "\n",
    "        Returns:\n",
    "            A list of transformed Documents.\n",
    "        \"\"\"\n",
    "def atransform_documents(self, documents: Sequence[Document], **kwargs: Any) -> Sequence[Document]:\n",
    "    \"\"\"Transform a list of documents.\n",
    "\n",
    "    Args:\n",
    "        documents: A sequence of Documents to be transformed.\n",
    "\n",
    "    Returns:\n",
    "        A list of transformed Documents.\n",
    "    \"\"\"\n",
    "    return self.transform_documents(documents, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"    async def atransform_documents(\n",
    "        self, documents: Sequence[Document], **kwargs: Any\n",
    "    ) -> Sequence[Document]:\n",
    "      Asynchronously transform a list of documents.\n",
    "\n",
    "        Args:\n",
    "            documents: A sequence of Documents to be transformed.\n",
    "\n",
    "        Returns:\n",
    "            A list of transformed Documents.\n",
    "    \n",
    "        return await asyncio.get_running_loop().run_in_executor(\n",
    "            None, partial(self.transform_documents, **kwargs), documents\n",
    "        )\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classe BaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLoader(ABC):\n",
    "    \"\"\"Interface for Document Loader.\n",
    "\n",
    "    Implementations should implement the lazy-loading method using generators\n",
    "    to avoid loading all Documents into memory at once.\n",
    "\n",
    "    The `load` method will remain as is for backwards compatibility, but its\n",
    "    implementation should be just `list(self.lazy_load())`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sub-classes should implement this method\n",
    "    # as return list(self.lazy_load()).\n",
    "    # This method returns a List which is materialized in memory.\n",
    "    @abstractmethod\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Load data into Document objects.\"\"\"\n",
    "\n",
    "    def load_and_split(\n",
    "        self, text_splitter: Optional[TextSplitter] = None\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Load Documents and split into chunks. Chunks are returned as Documents.\n",
    "\n",
    "        Args:\n",
    "            text_splitter: TextSplitter instance to use for splitting documents.\n",
    "              Defaults to RecursiveCharacterTextSplitter.\n",
    "\n",
    "        Returns:\n",
    "            List of Documents.\n",
    "        \"\"\"\n",
    "        if text_splitter is None:\n",
    "            _text_splitter: TextSplitter = RecursiveCharacterTextSplitter()\n",
    "            print(f'\\nBaseLoader - _text_splitter:\\n{_text_splitter}\\n\\n')\n",
    "        else:\n",
    "            _text_splitter = text_splitter\n",
    "        docs = self.load()\n",
    "        return _text_splitter.split_documents(docs)\n",
    "\n",
    "    # Attention: This method will be upgraded into an abstractmethod once it's\n",
    "    #            implemented in all the existing subclasses.\n",
    "    def lazy_load(\n",
    "        self,\n",
    "    ) -> Iterator[Document]:\n",
    "        \"\"\"A lazy loader for Documents.\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            f\"{self.__class__.__name__} does not implement lazy_load()\"\n",
    "        )\n",
    "\n",
    "\n",
    "class BaseBlobParser(ABC):\n",
    "    \"\"\"Abstract interface for blob parsers.\n",
    "\n",
    "    A blob parser provides a way to parse raw data stored in a blob into one\n",
    "    or more documents.\n",
    "\n",
    "    The parser can be composed with blob loaders, making it easy to reuse\n",
    "    a parser independent of how the blob was originally loaded.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def lazy_parse(self, blob: Blob) -> Iterator[Document]:\n",
    "        \"\"\"Lazy parsing interface.\n",
    "\n",
    "        Subclasses are required to implement this method.\n",
    "\n",
    "        Args:\n",
    "            blob: Blob instance\n",
    "\n",
    "        Returns:\n",
    "            Generator of documents\n",
    "        \"\"\"\n",
    "\n",
    "    def parse(self, blob: Blob) -> List[Document]:\n",
    "        \"\"\"Eagerly parse the blob into a document or documents.\n",
    "\n",
    "        This is a convenience method for interactive development environment.\n",
    "\n",
    "        Production applications should favor the lazy_parse method instead.\n",
    "\n",
    "        Subclasses should generally not over-ride this parse method.\n",
    "\n",
    "        Args:\n",
    "            blob: Blob instance\n",
    "\n",
    "        Returns:\n",
    "            List of documents\n",
    "        \"\"\"\n",
    "        return list(self.lazy_parse(blob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classe WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe WebBaseLoader sem o Async\n",
    "\n",
    "#Replace async def with def for methods that are asynchronous.\n",
    "#Replace await with synchronous calls for things like HTTP requests.\n",
    "# Remove asyncio specific constructs like asyncio.Semaphore, asyncio.gather, etc.\n",
    "\n",
    "\n",
    "import requests\n",
    "from typing import List, Any\n",
    "\n",
    "\n",
    "default_header_template = {\n",
    "    \"User-Agent\": \"\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*\"\n",
    "    \";q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"Referer\": \"https://www.google.com/\",\n",
    "    \"DNT\": \"1\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "}\n",
    "\n",
    "\n",
    "def _build_metadata(soup: Any, url: str) -> dict:\n",
    "    \"\"\"Build metadata from BeautifulSoup output.\"\"\"\n",
    "    metadata = {\"source\": url}\n",
    "    if title := soup.find(\"title\"):\n",
    "        metadata[\"title\"] = title.get_text()\n",
    "        print(f'\\nWebBaseLoader - title:\\n{metadata[\"title\"]}\\n\\n')\n",
    "    if description := soup.find(\"meta\", attrs={\"name\": \"description\"}):\n",
    "        metadata[\"description\"] = description.get(\"content\", \"No description found.\")\n",
    "        print(f'\\nWebBaseLoader - description:\\n{metadata[\"description\"]}\\n\\n')\n",
    "    if html := soup.find(\"html\"):\n",
    "        metadata[\"language\"] = html.get(\"lang\", \"No language found.\")\n",
    "    return metadata\n",
    "\n",
    "\n",
    "\n",
    "class WebBaseLoader(BaseLoader):\n",
    "    \"\"\"Load HTML pages using `urllib` and parse them with `BeautifulSoup'.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        web_path: Union[str, Sequence[str]] = \"\",\n",
    "        header_template: Optional[dict] = None,\n",
    "        verify_ssl: bool = True,\n",
    "        proxies: Optional[dict] = None,\n",
    "        continue_on_failure: bool = False,\n",
    "        autoset_encoding: bool = True,\n",
    "        encoding: Optional[str] = None,\n",
    "        web_paths: Sequence[str] = (),\n",
    "        requests_per_second: int = 2,\n",
    "        default_parser: str = \"html.parser\",\n",
    "        requests_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        raise_for_status: bool = False,\n",
    "        bs_get_text_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        bs_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        session: Any = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize loader.\n",
    "\n",
    "        Args:\n",
    "            web_paths: Web paths to load from.\n",
    "            requests_per_second: Max number of concurrent requests to make.\n",
    "            default_parser: Default parser to use for BeautifulSoup.\n",
    "            requests_kwargs: kwargs for requests\n",
    "            raise_for_status: Raise an exception if http status code denotes an error.\n",
    "            bs_get_text_kwargs: kwargs for beatifulsoup4 get_text\n",
    "            bs_kwargs: kwargs for beatifulsoup4 web page parsing\n",
    "        \"\"\"\n",
    "        # web_path kept for backwards-compatibility.\n",
    "        if web_path and web_paths:\n",
    "            raise ValueError(\n",
    "                \"Received web_path and web_paths. Only one can be specified. \"\n",
    "                \"web_path is deprecated, web_paths should be used.\"\n",
    "            )\n",
    "        if web_paths:\n",
    "            self.web_paths = list(web_paths)\n",
    "        elif isinstance(web_path, str):\n",
    "            self.web_paths = [web_path]\n",
    "        elif isinstance(web_path, Sequence):\n",
    "            self.web_paths = list(web_path)\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                f\"web_path must be str or Sequence[str] got ({type(web_path)}) or\"\n",
    "                f\" web_paths must be Sequence[str] got ({type(web_paths)})\"\n",
    "            )\n",
    "        self.requests_per_second = requests_per_second\n",
    "        self.default_parser = default_parser\n",
    "        self.requests_kwargs = requests_kwargs or {}\n",
    "        self.raise_for_status = raise_for_status\n",
    "        self.bs_get_text_kwargs = bs_get_text_kwargs or {}\n",
    "        self.bs_kwargs = bs_kwargs or {}\n",
    "        if session:\n",
    "            self.session = session\n",
    "        else:\n",
    "            session = requests.Session()\n",
    "            header_template = header_template or default_header_template.copy()\n",
    "            if not header_template.get(\"User-Agent\"):\n",
    "                try:\n",
    "                    from fake_useragent import UserAgent\n",
    "\n",
    "                    header_template[\"User-Agent\"] = UserAgent().random\n",
    "                except ImportError:\n",
    "                    logger.info(\n",
    "                        \"fake_useragent not found, using default user agent.\"\n",
    "                        \"To get a realistic header for requests, \"\n",
    "                        \"`pip install fake_useragent`.\"\n",
    "                    )\n",
    "            session.headers = dict(header_template)\n",
    "            session.verify = verify_ssl\n",
    "            if proxies:\n",
    "                session.proxies.update(proxies)\n",
    "            self.session = session\n",
    "        self.continue_on_failure = continue_on_failure\n",
    "        self.autoset_encoding = autoset_encoding\n",
    "        self.encoding = encoding\n",
    "\n",
    "    @property\n",
    "    def web_path(self) -> str:\n",
    "        if len(self.web_paths) > 1:\n",
    "            raise ValueError(\"Multiple webpaths found.\")\n",
    "        return self.web_paths[0]\n",
    "\n",
    "    def _fetch(self, url: str, retries: int = 3, cooldown: int = 2, backoff: float = 1.5) -> str:\n",
    "        for i in range(retries):\n",
    "            try:\n",
    "                response = requests.get(url, headers=self.session.headers, verify=self.session.verify)\n",
    "                # print(f'\\nWebBaseLoader - responde:\\n{response}\\n\\n')\n",
    "                if response.status_code == 200:\n",
    "                    return response.text\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                if i == retries - 1:\n",
    "                    raise\n",
    "                else:\n",
    "                    logger.warning(\n",
    "                        f\"Error fetching {url} with attempt \"\n",
    "                        f\"{i + 1}/{retries}: {e}. Retrying...\"\n",
    "                    )\n",
    "                    time.sleep(cooldown * backoff**i)\n",
    "        raise ValueError(\"retry count exceeded\")\n",
    "\n",
    "\n",
    "    def fetch_all(self, urls: List[str], max_pages: int = None) -> Any:\n",
    "        results = []\n",
    "        url_count = 0  # Contador para manter o controle do número de URLs processadas\n",
    "        \n",
    "        for url in urls:\n",
    "            if max_pages is not None and url_count >= max_pages:\n",
    "                break  # Sai do loop se o número máximo de páginas for alcançado\n",
    "                \n",
    "            try:\n",
    "                page_content = self._fetch(url)\n",
    "                print(f'\\nWebBaseLoader - page_content:\\n{page_content}\\n\\n')\n",
    "                results.append(page_content)\n",
    "                url_count += 1  # Incrementa o contador\n",
    "            except Exception as e:\n",
    "                if self.continue_on_failure:\n",
    "                    logger.warning(f\"Error fetching {url}, skipping due to continue_on_failure=True\")\n",
    "                else:\n",
    "                    raise e\n",
    "                    \n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_parser(parser: str) -> None:\n",
    "        \"\"\"Check that parser is valid for bs4.\"\"\"\n",
    "        valid_parsers = [\"html.parser\", \"lxml\", \"xml\", \"lxml-xml\", \"html5lib\"]\n",
    "        if parser not in valid_parsers:\n",
    "            raise ValueError(\n",
    "                \"`parser` must be one of \" + \", \".join(valid_parsers) + \".\"\n",
    "            )    \n",
    "\n",
    "    def _scrape(\n",
    "        self,\n",
    "        url: str,\n",
    "        parser: Union[str, None] = None,\n",
    "        bs_kwargs: Optional[dict] = None,\n",
    "    ) -> Any:\n",
    "        from bs4 import BeautifulSoup\n",
    "\n",
    "        if parser is None:\n",
    "            if url.endswith(\".xml\"):\n",
    "                parser = \"xml\"\n",
    "            else:\n",
    "                parser = self.default_parser\n",
    "\n",
    "        self._check_parser(parser)\n",
    "\n",
    "        html_doc = self.session.get(url, **self.requests_kwargs)\n",
    "        if self.raise_for_status:\n",
    "            html_doc.raise_for_status()\n",
    "\n",
    "        if self.encoding is not None:\n",
    "            html_doc.encoding = self.encoding\n",
    "        elif self.autoset_encoding:\n",
    "            html_doc.encoding = html_doc.apparent_encoding\n",
    "            \n",
    "        print(f'antes do return do _scrape: {BeautifulSoup(html_doc.text, parser, **(bs_kwargs or {}))}') ##### Bastante conteudo\n",
    "        return BeautifulSoup(html_doc.text, parser, **(bs_kwargs or {}))\n",
    "    \n",
    "\n",
    "    def scrape(self, parser: Union[str, None] = None) -> Any:\n",
    "        \"\"\"Scrape data from webpage and return it in BeautifulSoup format.\"\"\"\n",
    "\n",
    "        if parser is None:\n",
    "            parser = self.default_parser\n",
    "\n",
    "        return self._scrape(self.web_path, parser=parser, bs_kwargs=self.bs_kwargs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def scrape_all(self, urls: List[str], parser: Union[str, None] = None) -> List[Any]:\n",
    "        \"\"\"Fetch all urls, then return soups for all results.\"\"\"\n",
    "        from bs4 import BeautifulSoup\n",
    "\n",
    "        results = self.fetch_all(urls, max_pages=2)\n",
    "        final_results = []\n",
    "        for i, result in enumerate(results):\n",
    "            url = urls[i]\n",
    "            if parser is None:\n",
    "                if url.endswith(\".xml\"):\n",
    "                    parser = \"xml\"\n",
    "                else:\n",
    "                    parser = self.default_parser\n",
    "                self._check_parser(parser)\n",
    "            final_results.append(BeautifulSoup(result, parser, **self.bs_kwargs))\n",
    "        \n",
    "        print(f'\\nWebBaseLoader - final_results:\\n{final_results}\\n\\n')\n",
    "        return final_results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        \"\"\"Lazy load text from the url(s) in web_path.\"\"\"\n",
    "        for path in self.web_paths:\n",
    "            soup = self._scrape(path)\n",
    "            text = soup.get_text(**self.bs_get_text_kwargs)\n",
    "            metadata = _build_metadata(soup, path)\n",
    "            yield Document(page_content=text, metadata=metadata)\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Load text from the url(s) in web_path.\"\"\"\n",
    "        return list(self.lazy_load())\n",
    "\n",
    "\n",
    "    def aload(self) -> List[Document]:\n",
    "        \"\"\"Load text from the urls in web_path async into Documents.\"\"\"\n",
    "        results = self.scrape_all(self.web_paths)\n",
    "        docs = []\n",
    "        for path, soup in zip(self.web_paths, results):\n",
    "            text = soup.get_text(**self.bs_get_text_kwargs)\n",
    "            metadata = _build_metadata(soup, path)\n",
    "            docs.append(Document(page_content=text, metadata=metadata))\n",
    "            \n",
    "        print(f'\\nWebBaseLoader - docs:\\n{docs}\\n\\n')\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classe SitemapLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SitemapLoader(WebBaseLoader):\n",
    "    \"\"\"Load a sitemap and its URLs.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        web_path: str,\n",
    "        filter_urls: Optional[List[str]] = None,\n",
    "        parsing_function: Optional[Callable] = None,\n",
    "        blocksize: Optional[int] = None,\n",
    "        blocknum: int = 0,\n",
    "        meta_function: Optional[Callable] = None,\n",
    "        is_local: bool = False,\n",
    "        continue_on_failure: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        \"\"\"Initialize with webpage path and optional filter URLs.\n",
    "\n",
    "        Args:\n",
    "            web_path: url of the sitemap. can also be a local path\n",
    "            filter_urls: list of strings or regexes that will be applied to filter the\n",
    "                urls that are parsed and loaded\n",
    "            parsing_function: Function to parse bs4.Soup output\n",
    "            blocksize: number of sitemap locations per block\n",
    "            blocknum: the number of the block that should be loaded - zero indexed.\n",
    "                Default: 0\n",
    "            meta_function: Function to parse bs4.Soup output for metadata\n",
    "                remember when setting this method to also copy metadata[\"loc\"]\n",
    "                to metadata[\"source\"] if you are using this field\n",
    "            is_local: whether the sitemap is a local file. Default: False\n",
    "            continue_on_failure: whether to continue loading the sitemap if an error\n",
    "                occurs loading a url, emitting a warning instead of raising an\n",
    "                exception. Setting this to True makes the loader more robust, but also\n",
    "                may result in missing data. Default: False\n",
    "        \"\"\"\n",
    "\n",
    "        if blocksize is not None and blocksize < 1:\n",
    "            raise ValueError(\"Sitemap blocksize should be at least 1\")\n",
    "\n",
    "        if blocknum < 0:\n",
    "            raise ValueError(\"Sitemap blocknum can not be lower then 0\")\n",
    "\n",
    "        try:\n",
    "            import lxml  # noqa:F401\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"lxml package not found, please install it with \" \"`pip install lxml`\"\n",
    "            )\n",
    "\n",
    "        super().__init__(web_paths=[web_path], **kwargs)\n",
    "\n",
    "        self.filter_urls = filter_urls\n",
    "        self.parsing_function = parsing_function or _default_parsing_function\n",
    "        self.meta_function = meta_function or _default_meta_function\n",
    "        self.blocksize = blocksize\n",
    "        self.blocknum = blocknum\n",
    "        self.is_local = is_local\n",
    "        self.continue_on_failure = continue_on_failure\n",
    "\n",
    "    def parse_sitemap(self, soup: Any) -> List[dict]:\n",
    "        \"\"\"Parse sitemap xml and load into a list of dicts.\n",
    "\n",
    "        Args:\n",
    "            soup: BeautifulSoup object.\n",
    "\n",
    "        Returns:\n",
    "            List of dicts.\n",
    "        \"\"\"\n",
    "        els = []\n",
    "        for url in soup.find_all(\"url\"):\n",
    "            loc = url.find(\"loc\")\n",
    "            if not loc:\n",
    "                continue\n",
    "\n",
    "            # Strip leading and trailing whitespace and newlines\n",
    "            loc_text = loc.text.strip()\n",
    "\n",
    "            if self.filter_urls and not any(\n",
    "                re.match(r, loc_text) for r in self.filter_urls\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            els.append(\n",
    "                {\n",
    "                    tag: prop.text\n",
    "                    for tag in [\"loc\", \"lastmod\", \"changefreq\", \"priority\"]\n",
    "                    if (prop := url.find(tag))\n",
    "                }\n",
    "            )\n",
    "\n",
    "        for sitemap in soup.find_all(\"sitemap\"):\n",
    "            loc = sitemap.find(\"loc\")\n",
    "            print(f'\\n- Dentro de Sitemap - loc:\\n{loc}\\n\\n')\n",
    "            if not loc:\n",
    "                continue\n",
    "            soup_child = self.scrape_all([loc.text], \"xml\")[0]\n",
    "\n",
    "            els.extend(self.parse_sitemap(soup_child))\n",
    "        return els\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Load sitemap.\"\"\"\n",
    "        if self.is_local:\n",
    "            try:\n",
    "                import bs4\n",
    "            except ImportError:\n",
    "                raise ImportError(\n",
    "                    \"beautifulsoup4 package not found, please install it\"\n",
    "                    \" with `pip install beautifulsoup4`\"\n",
    "                )\n",
    "            fp = open(self.web_path)\n",
    "            soup = bs4.BeautifulSoup(fp, \"xml\")\n",
    "        else:\n",
    "            soup = self._scrape(self.web_path, parser=\"xml\")\n",
    "\n",
    "        els = self.parse_sitemap(soup)\n",
    "\n",
    "        if self.blocksize is not None:\n",
    "            elblocks = list(_batch_block(els, self.blocksize))\n",
    "            blockcount = len(elblocks)\n",
    "            if blockcount - 1 < self.blocknum:\n",
    "                raise ValueError(\n",
    "                    \"Selected sitemap does not contain enough blocks for given blocknum\"\n",
    "                )\n",
    "            else:\n",
    "                els = elblocks[self.blocknum]\n",
    "\n",
    "        results = self.scrape_all([el[\"loc\"].strip() for el in els if \"loc\" in el])\n",
    "\n",
    "        return [\n",
    "            Document(\n",
    "                page_content=self.parsing_function(results[i]),\n",
    "                metadata=self.meta_function(els[i], results[i]),\n",
    "            )\n",
    "            for i in range(len(results))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _default_parsing_function(content: Any) -> str:\n",
    "    return str(content.get_text())\n",
    "\n",
    "\n",
    "def _default_meta_function(meta: dict, _content: Any) -> dict:\n",
    "    return {\"source\": meta[\"loc\"], **meta}\n",
    "\n",
    "\n",
    "def _batch_block(iterable: Iterable, size: int) -> Generator[List[dict], None, None]:\n",
    "    it = iter(iterable)\n",
    "    while item := list(itertools.islice(it, size)):\n",
    "        yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><mark> langchain_docs_extractor </mark></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langchain_docs_extractor(soup: BeautifulSoup) -> str:\n",
    "    # Remove all the tags that are not meaningful for the extraction.\n",
    "    SCAPE_TAGS = [\"nav\", \"footer\", \"aside\", \"script\", \"style\"]\n",
    "    [tag.decompose() for tag in soup.find_all(SCAPE_TAGS)]\n",
    "\n",
    "    def get_text(tag: Tag) -> Generator[str, None, None]:\n",
    "        for child in tag.children:\n",
    "            if isinstance(child, Doctype):\n",
    "                continue\n",
    "\n",
    "            if isinstance(child, NavigableString):\n",
    "                yield child\n",
    "            elif isinstance(child, Tag):\n",
    "                if child.name in [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]:\n",
    "                    yield f\"{'#' * int(child.name[1:])} {child.get_text()}\\n\\n\"\n",
    "                elif child.name == \"a\":\n",
    "                    yield f\"[{child.get_text(strip=False)}]({child.get('href')})\"\n",
    "                elif child.name == \"img\":\n",
    "                    yield f\"![{child.get('alt', '')}]({child.get('src')})\"\n",
    "                elif child.name in [\"strong\", \"b\"]:\n",
    "                    yield f\"**{child.get_text(strip=False)}**\"\n",
    "                elif child.name in [\"em\", \"i\"]:\n",
    "                    yield f\"_{child.get_text(strip=False)}_\"\n",
    "                elif child.name == \"br\":\n",
    "                    yield \"\\n\"\n",
    "                elif child.name == \"code\":\n",
    "                    parent = child.find_parent()\n",
    "                    if parent is not None and parent.name == \"pre\":\n",
    "                        classes = parent.attrs.get(\"class\", \"\")\n",
    "\n",
    "                        language = next(\n",
    "                            filter(lambda x: re.match(r\"language-\\w+\", x), classes),\n",
    "                            None,\n",
    "                        )\n",
    "                        if language is None:\n",
    "                            language = \"\"\n",
    "                        else:\n",
    "                            language = language.split(\"-\")[1]\n",
    "\n",
    "                        lines: list[str] = []\n",
    "                        for span in child.find_all(\"span\", class_=\"token-line\"):\n",
    "                            print(f'\\nDentro de doc_extractor - span:\\n{span}\\n\\n')\n",
    "                            line_content = \"\".join(\n",
    "                                token.get_text() for token in span.find_all(\"span\")\n",
    "                            )\n",
    "                            lines.append(line_content)\n",
    "\n",
    "                        code_content = \"\\n\".join(lines)\n",
    "                        yield f\"```{language}\\n{code_content}\\n```\\n\\n\"\n",
    "                    else:\n",
    "                        yield f\"`{child.get_text(strip=False)}`\"\n",
    "\n",
    "                elif child.name == \"p\":\n",
    "                    yield from get_text(child)\n",
    "                    yield \"\\n\\n\"\n",
    "                elif child.name == \"ul\":\n",
    "                    for li in child.find_all(\"li\", recursive=False):\n",
    "                        yield \"- \"\n",
    "                        yield from get_text(li)\n",
    "                        yield \"\\n\\n\"\n",
    "                elif child.name == \"ol\":\n",
    "                    for i, li in enumerate(child.find_all(\"li\", recursive=False)):\n",
    "                        yield f\"{i + 1}. \"\n",
    "                        yield from get_text(li)\n",
    "                        yield \"\\n\\n\"\n",
    "                elif child.name == \"div\" and \"tabs-container\" in child.attrs.get(\n",
    "                    \"class\", [\"\"]\n",
    "                ):\n",
    "                    tabs = child.find_all(\"li\", {\"role\": \"tab\"})\n",
    "                    tab_panels = child.find_all(\"div\", {\"role\": \"tabpanel\"})\n",
    "                    for tab, tab_panel in zip(tabs, tab_panels):\n",
    "                        tab_name = tab.get_text(strip=True)\n",
    "                        yield f\"{tab_name}\\n\"\n",
    "                        yield from get_text(tab_panel)\n",
    "                elif child.name == \"table\":\n",
    "                    thead = child.find(\"thead\")\n",
    "                    header_exists = isinstance(thead, Tag)\n",
    "                    if header_exists:\n",
    "                        headers = thead.find_all(\"th\")\n",
    "                        if headers:\n",
    "                            yield \"| \"\n",
    "                            yield \" | \".join(header.get_text() for header in headers)\n",
    "                            yield \" |\\n\"\n",
    "                            yield \"| \"\n",
    "                            yield \" | \".join(\"----\" for _ in headers)\n",
    "                            yield \" |\\n\"\n",
    "\n",
    "                    tbody = child.find(\"tbody\")\n",
    "                    tbody_exists = isinstance(tbody, Tag)\n",
    "                    if tbody_exists:\n",
    "                        for row in tbody.find_all(\"tr\"):\n",
    "                            yield \"| \"\n",
    "                            yield \" | \".join(\n",
    "                                cell.get_text(strip=True) for cell in row.find_all(\"td\")\n",
    "                                \n",
    "                            )\n",
    "                            yield \" |\\n\"\n",
    "\n",
    "                    yield \"\\n\\n\"\n",
    "                elif child.name in [\"button\"]:\n",
    "                    continue\n",
    "                else:\n",
    "                    yield from get_text(child)\n",
    "\n",
    "    joined = \"\".join(get_text(soup))\n",
    "    print(\"Dentro da funçao langchain_docs_extractor:\")\n",
    "    \n",
    "    print(re.sub(r\"\\n\\n+\", \"\\n\\n\", joined).strip())\n",
    "    print()\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", joined).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def _metadata_extractor(raw_html: str, url: str) -> dict:\n",
    "    \"\"\"Extract metadata from raw html using BeautifulSoup.\"\"\"\n",
    "    metadata = {\"source\": url}\n",
    "\n",
    "    try:\n",
    "        from bs4 import BeautifulSoup\n",
    "    except ImportError:\n",
    "        logger.warning(\n",
    "            \"The bs4 package is required for default metadata extraction. \"\n",
    "            \"Please install it with `pip install bs4`.\"\n",
    "        )\n",
    "        return metadata\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    if title := soup.find(\"title\"):\n",
    "        metadata[\"title\"] = title.get_text()\n",
    "    if description := soup.find(\"meta\", attrs={\"name\": \"description\"}):\n",
    "        metadata[\"description\"] = description.get(\"content\", None)\n",
    "    if html := soup.find(\"html\"):\n",
    "        metadata[\"language\"] = html.get(\"lang\", None)\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Classe RecursiveURLLoader</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utils.html import extract_sub_links\n",
    "\n",
    "class RecursiveUrlLoader(BaseLoader):\n",
    "    \"\"\"Load all child links from a URL page.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        url: str,\n",
    "        max_depth: Optional[int] = 2,\n",
    "        extractor: Optional[Callable[[str], str]] = None,\n",
    "        metadata_extractor: Optional[Callable[[str, str], str]] = None,\n",
    "        exclude_dirs: Optional[Sequence[str]] = (),\n",
    "        timeout: Optional[int] = 10,\n",
    "        prevent_outside: Optional[bool] = True,\n",
    "        link_regex: Union[str, re.Pattern, None] = None,\n",
    "        headers: Optional[dict] = None,\n",
    "        check_response_status: bool = False,\n",
    "    ) -> None:\n",
    "\n",
    "        self.url = url\n",
    "        self.max_depth = max_depth if max_depth is not None else 2\n",
    "        self.extractor = extractor if extractor is not None else lambda x: x\n",
    "        self.metadata_extractor = (\n",
    "            metadata_extractor\n",
    "            if metadata_extractor is not None\n",
    "            else _metadata_extractor\n",
    "        )\n",
    "        self.exclude_dirs = exclude_dirs if exclude_dirs is not None else ()\n",
    "        self.timeout = timeout\n",
    "        self.prevent_outside = prevent_outside if prevent_outside is not None else True\n",
    "        self.link_regex = link_regex\n",
    "        self.headers = headers\n",
    "        self.check_response_status = check_response_status\n",
    "\n",
    "    def _get_child_links_recursive(\n",
    "        self, url: str, visited: Set[str], *, depth: int = 0\n",
    "    ) -> Iterator[Document]:\n",
    "          \n",
    "        if depth >= self.max_depth:\n",
    "            return\n",
    "\n",
    "        # Get all links that can be accessed from the current URL\n",
    "        visited.add(url)\n",
    "        try:\n",
    "            response = requests.get(url, timeout=self.timeout, headers=self.headers)\n",
    "            if self.check_response_status and 400 <= response.status_code <= 599:\n",
    "                raise ValueError(f\"Received HTTP status {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(\n",
    "                f\"Unable to load from {url}. Received error {e} of type \"\n",
    "                f\"{e.__class__.__name__}\"\n",
    "            )\n",
    "            return\n",
    "        content = self.extractor(response.text)\n",
    "        if content:\n",
    "            yield Document(\n",
    "                page_content=content,\n",
    "                metadata=self.metadata_extractor(response.text, url),\n",
    "            )\n",
    "\n",
    "        # Store the visited links and recursively visit the children\n",
    "        sub_links = extract_sub_links(\n",
    "            response.text,\n",
    "            url,\n",
    "            base_url=self.url,\n",
    "            pattern=self.link_regex,\n",
    "            prevent_outside=self.prevent_outside,\n",
    "            exclude_prefixes=self.exclude_dirs,\n",
    "        )\n",
    "        max_link = 5\n",
    "        url_link_count = 0\n",
    "        for link in sub_links:\n",
    "            print(f'\\nurl_link_count: {url_link_count}\\n\\n')\n",
    "            #if url_link_count >= max_link:\n",
    "                #break  # Sai do loop\n",
    "            # Check all unvisited links\n",
    "            if link not in visited:\n",
    "                yield from self._get_child_links_recursive(\n",
    "                    link, visited, depth=depth + 1\n",
    "                )\n",
    "            url_link_count += 1    \n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        visited: Set[str] = set()\n",
    "        return self._get_child_links_recursive(self.url, visited)\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Load web pages.\"\"\"\n",
    "        return list(self.lazy_load())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark> Ingest </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadata_extractor(meta: dict, soup: BeautifulSoup) -> dict:\n",
    "    title = soup.find(\"title\")\n",
    "    print(f'\\nIngest - metadata_extractor - title:\\n{title}\\n\\n')\n",
    "    description = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "    print(f'\\nIngest - metadata_extractor - description:\\n{description}\\n\\n')\n",
    "    html = soup.find(\"html\")\n",
    "    return {\n",
    "        \"source\": meta[\"loc\"],\n",
    "        \"title\": title.get_text() if title else \"\",\n",
    "        \"description\": description.get(\"content\", \"\") if description else \"\",\n",
    "        \"language\": html.get(\"lang\", \"\") if html else \"\",\n",
    "        **meta,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# https://python.langchain.com/sitemap.xml   / https://python.langchain.com/\n",
    "\n",
    "def load_langchain_docs():\n",
    "    return SitemapLoader(\n",
    "        \"https://transparencia.mesquita.rj.gov.br/\",\n",
    "        filter_urls=[\"https://transparencia.mesquita.rj.gov.br/\"],\n",
    "        parsing_function=langchain_docs_extractor,\n",
    "        default_parser=\"lxml\",\n",
    "        bs_kwargs={\n",
    "            \"parse_only\": SoupStrainer(\n",
    "                name=(\"article\", \"title\", \"html\", \"lang\", \"content\")\n",
    "            ),\n",
    "        },\n",
    "        meta_function=metadata_extractor,\n",
    "    ).load()\n",
    "\n",
    "def simple_extractor(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_api_docs():\n",
    "    return RecursiveUrlLoader(\n",
    "        url=\"https://api.python.langchain.com/en/latest/\",\n",
    "        max_depth=8,\n",
    "        extractor=simple_extractor,\n",
    "        prevent_outside=True,\n",
    "        timeout=600,\n",
    "        # Drop trailing / to avoid duplicate pages.\n",
    "        link_regex=(\n",
    "            f\"href=[\\\"']{PREFIXES_TO_IGNORE_REGEX}((?:{SUFFIXES_TO_IGNORE_REGEX}.)*?)\"\n",
    "            r\"(?:[\\#'\\\"]|\\/[\\#'\\\"])\"\n",
    "        ),\n",
    "        check_response_status=True,\n",
    "        exclude_dirs=(\n",
    "            \"https://api.python.langchain.com/en/latest/_sources\",\n",
    "            \"https://api.python.langchain.com/en/latest/_modules\",\n",
    "        ),\n",
    "    ).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_docs_jupyter():\n",
    "    docs_from_documentation = load_langchain_docs()\n",
    "    logger.info(f\"Loaded {len(docs_from_documentation)} docs from documentation\")\n",
    "    # docs_from_api = load_api_docs()\n",
    "    # logger.info(f\"Loaded {len(docs_from_api)} docs from API\")\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=200)\n",
    "    docs_transformed = text_splitter.split_documents(\n",
    "        docs_from_documentation\n",
    "    )\n",
    "    \n",
    "    #docs_from_documentation + docs_from_api\n",
    "\n",
    "    # We try to return 'source' and 'title' metadata when querying vector store and\n",
    "    # Weaviate will error at query time if one of the attributes is missing from a\n",
    "    # retrieved document.\n",
    "    num_docs = 0\n",
    "    max_doc = 10\n",
    "    for doc in docs_transformed:\n",
    "        if num_docs >= max_doc:\n",
    "            break\n",
    "        print(f'\\ndoc.metadata[\"source\"]: {doc.metadata[\"source\"]}')\n",
    "        print(f'\\ndoc.metadata[\"title\"]: {doc.metadata[\"title\"]}')\n",
    "        if \"source\" not in doc.metadata:\n",
    "            doc.metadata[\"source\"] = \"\"\n",
    "        if \"title\" not in doc.metadata:\n",
    "            doc.metadata[\"title\"] = \"\"\n",
    "        num_docs += 1\n",
    "    # client = weaviate.Client(\n",
    "    #     url=WEAVIATE_URL,\n",
    "    #     auth_client_secret=weaviate.AuthApiKey(api_key=WEAVIATE_API_KEY),\n",
    "    # )\n",
    "    # embedding = OpenAIEmbeddings(\n",
    "    #     chunk_size=200,\n",
    "    # )  # rate limit\n",
    "    # vectorstore = Weaviate(\n",
    "    #     client=client,\n",
    "    #     index_name=WEAVIATE_DOCS_INDEX_NAME,\n",
    "    #     text_key=\"text\",\n",
    "    #     embedding=embedding,\n",
    "    #     by_text=False,\n",
    "    #     attributes=[\"source\", \"title\"],\n",
    "    # )\n",
    "\n",
    "    # record_manager = SQLRecordManager(\n",
    "    #     f\"weaviate/{WEAVIATE_DOCS_INDEX_NAME}\", db_url=RECORD_MANAGER_DB_URL\n",
    "    # )\n",
    "    # record_manager.create_schema()\n",
    "\n",
    "    # indexing_stats = index(\n",
    "    #     docs_transformed,\n",
    "    #     record_manager,\n",
    "    #     vectorstore,\n",
    "    #     cleanup=\"full\",\n",
    "    #     source_id_key=\"source\",\n",
    "    # )\n",
    "\n",
    "    # logger.info(\"Indexing stats: \", indexing_stats)\n",
    "    # logger.info(\n",
    "    #     \"LangChain now has this many vectors: \",\n",
    "    #     client.query.aggregate(WEAVIATE_DOCS_INDEX_NAME).with_meta_count().do(),\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antes do return do _scrape: <?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
      "\n",
      "\n",
      "WebBaseLoader - final_results:\n",
      "[]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ingest_docs_jupyter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat-langchain-kK18R945-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
